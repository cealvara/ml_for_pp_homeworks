{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "Carlos E. Alvarado <br>\n",
    "Machine Learning for Public Policy <br>\n",
    "Spring 2017 <br>\n",
    "Repo: https://github.com/cealvara/ml_for_pp_homeworks.git\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"> \n",
    "<font size=\"6\">Homework 3</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Models selected\n",
    "\n",
    "For this homework, I ran several versions of the following models:\n",
    "\n",
    "- Random Forests\n",
    "- Bagging\n",
    "- AdaBoost\n",
    "- Logistic Regression\n",
    "- Linear SVM\n",
    "- Decision Tree\n",
    "\n",
    "I did not run (non-linear) SVM or KNN because, when testing, it took too much time to get results back from them.\n",
    "\n",
    "Importantly, several of the models I ran delivered the exact same results as other fitted models. This happens mainly in the case of Ensemble Models, because they are built on top of other models (e.g. Decision Trees), and under some configuration of parameters the models turned out to be the same as the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Features Selection\n",
    "\n",
    "After several iterations, I noticed most models improve significantly after including relevant features into the analysis. The following table summarizes the different iterations:\n",
    "\n",
    "**Base Vars**: *NumberOfDependents; age; MonthlyIncome Category Dummies; DebtRatio Category Dummies*\n",
    "       \n",
    "**Add. Vars 1**: *RevolvingUtilizationOfUnsecuredLines; NumberOfTimes90DaysLate*\n",
    "\n",
    "**Add. Vars 2**: *NumberOfOpenCreditLinesAndLoans; NumberOfTimes90DaysLate; NumberRealEstateLoansOrLines; NumberOfTime60-89DaysPastDueNotWorse*.\n",
    "\n",
    "<div style=\"text-align: center\"> \n",
    "<font size=\"4\"><br>Average AUC for Best 10 models\n",
    "</div>\n",
    "\n",
    "Features Included | Average AUC\n",
    "---- | :----:\n",
    "Base Vars\t| 63%\n",
    "Including Add. Vars 1 |\t80%\n",
    "Including Add. Vars 2\t| 82%\n",
    "\n",
    "For the reason described above, and in order to use as much information as possible, the following results include all Base Vars, Vars 1 and Vars 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Best models\n",
    "\n",
    "The following table summarizes the best 10 models according to AUC, using all features:\n",
    "\n",
    "<div style=\"text-align: center\"> \n",
    "<font size=\"4\"><br> 10 Best Models according to AUC\n",
    "</div>\n",
    "\n",
    "model_type | parameters | auc-roc | p_at_5 | p_at_10 | p_at_20 | elapsed\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "AB | {'n_estimators': 100, 'algorithm': 'SAMME.R'} | 0.84 | 0.47 | 0.36 | 0.24 | 3.49\n",
    "AB | {'n_estimators': 100, 'algorithm': 'SAMME'} | 0.83 | 0.46 | 0.36 | 0.23 | 2.56\n",
    "RF | {'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 100, 'max_features': 'log2'} | 0.82 | 0.47 | 0.35 | 0.23 | 7.23\n",
    "RF | {'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 100, 'max_features': 'sqrt'} | 0.82 | 0.46 | 0.35 | 0.23 | 6.75\n",
    "DT | {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2'} | 0.82 | 0.47 | 0.34 | 0.23 | 0.27\n",
    "RF | {'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 100, 'max_features': 'log2'} | 0.82 | 0.47 | 0.35 | 0.23 | 7.01\n",
    "RF | {'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 100, 'max_features': 'sqrt'} | 0.82 | 0.47 | 0.34 | 0.23 | 7.44\n",
    "DT | {'min_samples_split': 10, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2'} | 0.81 | 0.44 | 0.34 | 0.24 | 0.26\n",
    "RF | {'min_samples_split': 5, 'criterion': 'entropy', 'n_estimators': 100, 'max_features': 'log2'} | 0.81 | 0.45 | 0.34 | 0.23 | 7.68\n",
    "RF | {'min_samples_split': 5, 'criterion': 'gini', 'n_estimators': 100, 'max_features': 'sqrt'} | 0.81 | 0.45 | 0.33 | 0.23 | 5.97\n",
    "\n",
    "As we can see, the best models include AdaBoost, Random Forests and Decision Trees.\n",
    "\n",
    "<div style=\"text-align: center\"> \n",
    "<font size=\"4\"><br> 10 Best Models according to Precision at 10%\n",
    "</div>\n",
    "\n",
    "model_type | parameters | auc-roc | p_at_5 | p_at_10 | p_at_20 | elapsed\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "BA | {'max_samples': 1, 'n_estimators': 1, 'max_features': 0.5} | 0.5 | 1 | 0.71 | 0.35 | 0.18\n",
    "DT | {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 1, 'max_features': 'log2'} | 0.51 | 0.85 | 0.71 | 0.35 | 0.16\n",
    "BA | {'max_samples': 0.5, 'n_estimators': 1, 'max_features': 0.5} | 0.66 | 0.49 | 0.68 | 0.35 | 0.21\n",
    "BA | {'max_samples': 0.5, 'n_estimators': 1, 'max_features': 2} | 0.62 | 0.4 | 0.65 | 0.35 | 0.18\n",
    "DT | {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 1, 'max_features': 'log2'} | 0.53 | 1 | 0.61 | 0.31 | 0.17\n",
    "DT | {'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 1, 'max_features': 'sqrt'} | 0.58 | 1 | 0.56 | 0.28 | 0.17\n",
    "BA | {'max_samples': 2, 'n_estimators': 10, 'max_features': 0.5} | 0.49 | 1 | 0.55 | 0.28 | 2.94\n",
    "BA | {'max_samples': 2, 'n_estimators': 10, 'max_features': 2} | 0.5 | 0.98 | 0.51 | 0.26 | 2.95\n",
    "RF | {'min_samples_split': 2, 'criterion': 'entropy', 'n_estimators': 1, 'max_features': 'log2'} | 0.59 | 0.33 | 0.49 | 0.35 | 0.41\n",
    "DT | {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 100, 'max_features': 'log2'} | 0.6 | 0.35 | 0.48 | 0.35 | 0.31\n",
    "DT | {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 100, 'max_features': 'sqrt'} | 0.6 | 0.36 | 0.48 | 0.35 | 0.3\n",
    "\n",
    "Again, the best models include Random Forests and Decision Trees. Also, Bagging appears on the list. Note that AUC is very low for these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Observations about the results\n",
    "\n",
    "- For AdaBoost and Bagging models, they have a significant increase in AUC when moving from n_estimators=10 to 100.\n",
    "- However, increasing the number of estimators also increase running time.\n",
    "- The parameter \"max_features\" does not seem to play a significant role in the fitted models.\n",
    "- For Decision Trees models, the parameter \"criterion\" does not changes significantly the results.\n",
    "- Decision Trees models have a very small running time compare to other models.\n",
    "- Logistic Regression models seem to be the worst performing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would recommend using Random Forests or Decision Trees, because they have good performance and, also, they are relatively simple to interpret and explain to others. They have further parameters that can be modified to get more precise predictions, such as different class weights to deal with balance problems in the data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
